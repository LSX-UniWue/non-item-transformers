parser: &id001
  delimiter: "\t"
  item_column_name: item_id
  item_separator: ' + '
loader: &id002
  batch_size: 4
  max_seq_length: 10
  max_seq_step_length: 5
datasets:
  test:
    dataset:
      csv_file: /Users/nosebrain/Desktop/small/test.csv
      csv_file_index: /Users/nosebrain/Desktop/small/test.idx
      parser: *id001
      nip_index_file: /Users/nosebrain/Desktop/small/test.nip.idx
    loader: *id002
  train:
    dataset:
      csv_file: /Users/nosebrain/Desktop/small/train.csv
      csv_file_index: /Users/nosebrain/Desktop/small/train.idx
      parser: *id001
    loader:  *id002
  validation:
    dataset:
      csv_file: /Users/nosebrain/Desktop/small/valid.csv
      csv_file_index: /Users/nosebrain/Desktop/small/valid.idx
      parser: *id001
      nip_index_file: /Users/nosebrain/Desktop/small/valid.nip.idx
    loader: *id002
tokenizer:
  special_tokens:
    pad_token: <PAD>
    unk_token: <UNK>
  vocabulary:
    delimiter: "\t"
    file: /Users/nosebrain/Desktop/small/items.vocab
model:
  transformer_hidden_size: 64
  num_transformer_heads: 4
  num_transformer_layers: 3
  item_vocab_size: 250
  max_seq_length: 100
  dropout: 0.1
  embedding_mode: sum
module:
  learning_rate: 0.001
  beta_1: 0.99
  beta_2: 0.998
  batch_first: true
  metrics:
    recall:
      - 1
      - 3
      - 5
      - 10
    precision:
      - 1
      - 3
      - 5
      - 10
trainer:
  default_root_dir: /tmp/checkpoints
  checkpoints:
    monitor: f1_at_5
    save_top_k: 3