import torch

from typing import Union, List, Dict, Any, Set

from data.datasets import ITEM_SEQ_ENTRY_NAME, SAMPLE_IDS, POSITIVE_SAMPLES_ENTRY_NAME, NEGATIVE_SAMPLES_ENTRY_NAME
from data.datasets.processors.processor import Processor
from asme.tokenization.tokenizer import Tokenizer


class PositiveNegativeSamplerProcessor(Processor):
    """
    Takes the input sequence and generates positive and negative target
    sequences (:code data.datasets.{POSITIVE, NEGATIVE}_SAMPLES_ENTRY_NAME).

    The input sequence is updated by shifting it one to the right.
    The positive samples are generated by shifting the input sequence one to the left.
    For the negative samples, tokens are drawn uniformly random from the set of tokens not used during the session.

    Example:
        Input:
            sequence: [1, 5, 7, 8]
        Output:
            sequence:         [1, 5, 7]
            positive samples: [5, 7, 8]
            negative samples: [2, 9, 6]

    """
    def __init__(self,
                 tokenizer: Tokenizer
                 ):
        super().__init__()
        self._tokenizer = tokenizer

    def _sample_negative_target(self,
                                sequence: Union[List[int], List[List[int]]]
                                ) -> Union[List[int], List[List[int]]]:

        # we want to use a multinomial distribution to draw negative samples fast
        # start with a uniform distribution over all vocabulary tokens
        weights = torch.ones([len(self._tokenizer)])

        # set weight for special tokens to 0.
        weights[self._tokenizer.get_special_token_ids()] = 0.

        # prevent sampling of tokens already present in the session
        used_tokens = self._get_all_tokens_of_sequence(sequence)
        weights[list(used_tokens)] = 0.

        if isinstance(sequence[0], list):
            results = []
            for seq_step in sequence[:-1]:  # skip last target
                neg_samples = torch.multinomial(weights, num_samples=len(seq_step), replacement=True).tolist()
                results.append(neg_samples)
            return results

        return torch.multinomial(weights, num_samples=len(sequence) - 1, replacement=True).tolist()

    def _get_all_tokens_of_sequence(self,
                                    sequence: Union[List[int], List[List[int]]]
                                    ) -> Set[int]:
        if isinstance(sequence[0], list):
            flat_items = [item for sublist in sequence for item in sublist]
        else:
            flat_items = sequence

        return set(flat_items)

    def process(self,
                parsed_sequence: Dict[str, Any]
                ) -> Dict[str, Any]:
        sequence = parsed_sequence[ITEM_SEQ_ENTRY_NAME]

        if len(sequence) == 1:
            print(sequence)
            raise AssertionError(f'{parsed_sequence[SAMPLE_IDS]}')

        x = sequence[:-1]
        pos = sequence[1:]
        neg = self._sample_negative_target(sequence)

        parsed_sequence[ITEM_SEQ_ENTRY_NAME] = x
        parsed_sequence[POSITIVE_SAMPLES_ENTRY_NAME] = pos
        parsed_sequence[NEGATIVE_SAMPLES_ENTRY_NAME] = neg
        return parsed_sequence
