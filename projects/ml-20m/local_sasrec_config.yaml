parser: &parser
  item_column_name: title
  additional_features:
    userId:
      type: int
      sequence: false
loader: &loader
  batch_size: 4
  max_seq_length: 200
datasets:
  test:
    dataset:
      csv_file: ../dataset/dataset/ml-20m_3_5_5/ml-20m.csv
      csv_file_index: ../dataset/dataset/ml-20m_3_5_5/ml-20m.idx
      parser: *parser
      nip_index_file: ../dataset/dataset/ml-20m_3_5_5/test.loo.idx
    loader: *loader
  train:
    dataset:
      csv_file: ../dataset/dataset/ml-20m_3_5_5/ml-20m.csv
      csv_file_index: ../dataset/dataset/ml-20m_3_5_5/ml-20m.idx
      truncated_seq_index_file: ../dataset/dataset/ml-20m_3_5_5/valid.loo.idx
      parser: *parser
    loader: *loader
  validation:
    dataset:
      csv_file: ../dataset/dataset/ml-20m_3_5_5/ml-20m.csv
      csv_file_index: ../dataset/dataset/ml-20m_3_5_5/ml-20m.idx
      nip_index_file: ../dataset/dataset/ml-20m_3_5_5/valid.loo.idx
      parser: *parser
    loader: *loader
model:
  item_vocab_size: 18340
  max_seq_length: 200
  num_transformer_heads: 2
  num_transformer_layers: 2
  transformer_hidden_size: 64
  transformer_dropout: 0.1
module:
  mask_probability: 0.2
  beta_1: 0.9
  beta_2: 0.999
  learning_rate: 0.001
  weight_decay: 0.01
  num_warmup_steps: 100
  sampled_metrics:
    sample_probability_file: ../dataset/dataset/ml-20m_3_5_5/popularity.txt
    num_negative_samples: 100
    metrics:
      recall:
        - 1
        - 5
        - 10
tokenizer:
  special_tokens:
    pad_token: <PAD>
    mask_token: <MASK>
    unk_token: <UNK>
  vocabulary:
    delimiter: "\t"
    file: ../dataset/dataset/ml-20m_3_5_5/vocab_title.txt
trainer:
  limit_train_batches: 1000
  checkpoints:
    monitor: recall_at_5
    save_top_k: 3
  gradient_clip_val: 5
  default_root_dir: ../dataset/experiments/ml-20m/bert4rec
