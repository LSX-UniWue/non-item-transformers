parser: &parser
  item_column_name: item_id
  item_separator: ' + '
loader: &loader
  batch_size: 2
  max_seq_length: 5
  max_seq_step_length: 5
datasets:
  test:
    dataset:
      csv_file: ../tests/example_dataset/train.csv
      csv_file_index: ../tests/example_dataset/train.idx
      nip_index_file: ../tests/example_dataset/train.nip.idx
      parser: *parser
    loader: *loader
  train:
    dataset:
      csv_file: ../tests/example_dataset/train.csv
      csv_file_index: ../tests/example_dataset/train.idx
      parser: *parser
    loader: *loader
  validation:
    dataset:
      csv_file: ../tests/example_dataset/train.csv
      csv_file_index: ../tests/example_dataset/train.idx
      nip_index_file: ../tests/example_dataset/train.nip.idx
      parser: *parser
    loader: *loader

model:
  num_users: 0
  num_items: 0
  seq_len: 5
  # chosen from {10, 20, 30, 50, 100}, ML-1M: 50, Gowalla: 100
  embed_dim: 50
  block_num: 0
  block_dim: 0
  fc_dim: 0
  ac_fc: 'relu'
  drop_prop: 0.5

module:
  learning_rate: 0.001
  weight_decay: 0.01
  metrics:
    precision:
      - 1
      - 3
      - 5
    recall:
      - 1
      - 3
      - 5
    f1:
      - 1
      - 3
      - 5
tokenizer:
  special_tokens:
    pad_token: <PAD>
    mask_token: <MASK>
    unk_token: <UNK>
  vocabulary:
    delimiter: "\t"
    file: ../tests/example_dataset/vocab.txt
trainer:
  checkpoint:
    monitor: recall_at_5
    save_top_k: 3
