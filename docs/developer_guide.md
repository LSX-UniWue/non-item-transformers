# Code Overview #
After installing the virtual environment as described in the [Project Overview](./project_overview.md) setup your IDE
to use the virtual enviroment generated by Poetry. It is commonly found under `recommender/venv/recommender`.

The project is structured into 14 folders:
* [data](./../data): Contains code regarding the indexing of data sets and reading of stored indices
* [dataset](./../dataset): Contains code regarding the integration of data sets into the framework
* [dm](./../dm): Contains code for the use of the dota data set
* [docs](./../docs): Contains Markdown files which contain the documentation for this project
* [k8s](./../k8s): Contains a dockerfile which defines a docker container which runs a model when started, a kubernetes 
    job and pod configuration for said dockerfile.
* [logger](./../logger): Contains code for the logging of pytorch training loss and pytorch training metrics
* [losses](./../losses): Contains code regarding custom loss functions which can be integrated into pytorch training
* [metrics](./../metrics): Contains code reagarding the implementation of recommendation metrics which are integrated 
    using pytorch
* [models](./../models): Contains pytorch implementations of the models provided by this framework
* [modules](./../modules): Contains pytorch-lightning wrappers for the pytorch models defined in [models](./../models)
* [projects](./../projects): Contains yaml files which are needed to configure the uses of models in conjunction with
    metrics and data sets.
* [runner](./../runner): Also contains Typer-CLI implementation for indexing data sets, but also the Typer-CLI
    implementation for training a model and performing predictions with it
* [tests](./../tests)

## Adding a new Metric ##


## Adding a new Data set ##
All Necessary Files for the integration of data sets can be found under [data](./../data), [dataset](./../dataset), and
[runner/dataset](./../runner/dataset).

In order to add a new data set and make it usable you have to implement the following steps.
1. Download the data set or document where it is available
2. Index the data set and create train, validation, test split
3. Create CLI for execution of the newly implemented Code using [Typer](https://typer.tiangolo.com/)

## Adding a new Model ##
